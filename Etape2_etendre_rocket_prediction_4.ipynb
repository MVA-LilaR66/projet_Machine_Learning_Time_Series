{"cells":[{"cell_type":"markdown","metadata":{"id":"Rl8f3Ktz1saL"},"source":["# Projet Machine Learning for Time Series : ROCKET\n","\n","## Etudiants : Lila Roig & Eva Robillard\n","\n","Dans ce Notebook, nous étendons l'algorithme Rocket à la tache de prédiction. Rocket a initialement été créé et testé sur des taches de classification de séries temporelles univariées."]},{"cell_type":"markdown","metadata":{"id":"SKClAv2-1v5q"},"source":["## Biblio"]},{"cell_type":"markdown","metadata":{"id":"RcPfsFYa1mi4"},"source":["**Papiers**\n","\n","- Rocket paper : https://arxiv.org/pdf/1910.13051\n","\n","- Rocket official code : https://github.com/angus924/rocket\n","\n","- Comment préparer les Séries temporelles : https://openreview.net/pdf?id=wEc1mgAjU-\n","\n","- MiniRocket paper : https://arxiv.org/pdf/2012.08791\n","\n","- MiniRocket official code : https://github.com/angus924/minirocket/tree/main\n","\n","- MiniRocket tuto prise en main : https://github.com/timeseriesAI/tsai/blob/main/tutorial_nbs/10_Time_Series_Classification_and_Regression_with_MiniRocket.ipynb\n","\n","- MultiRocket paper : https://arxiv.org/abs/2102.00457\n","\n","---\n","**Classification Dataset**\n","\n","**UCR Archive** (dataset sur lequel a été testé Rocket dans le papier original)\n","\n","- UCR archive utilisée pour \"entraîner\" Rocket :  https://www.cs.ucr.edu/~eamonn/time_series_data_2018/ (répo officiel) et  http://www.timeseriesclassification.com/index.php (répo regroupant UCR acrhives et d'autres méthodes)\n","\n","- UCR archive paper : https://arxiv.org/pdf/1810.07758\n","\n","---\n","**Prediction Dataset**\n","\n","**Monash Time Series Forecasting Archive** (dataset que nous allons utiliser pour tester Rocket sur la prédiction)\n","\n","- Monash Dataset : https://forecastingdata.org/ (JE ME SUIS ARRETEE A BITCOIN INCLU)\n","- Monach paper dataset : https://openreview.net/pdf?id=wEc1mgAjU-\n","- Monach github : https://github.com/rakshitha123/TSForecasting?tab=readme-ov-file\n","- Monash Kaggle : https://www.kaggle.com/datasets/konradb/monash-time-series-forecasting-repository\n","- Monash Hugging Face : https://huggingface.co/datasets/Monash-University/monash_tsf\n","- Comment preprocesser les données : https://arxiv.org/pdf/1909.00590\n","\n"]},{"cell_type":"markdown","source":["**Attention !**\n","\n","Dans le papier du Monash dataset, ils ne traitent pas dans leur test les datasets suivants pour des raisons de coût de calcul. Comme nous n'avons pas de machine très puissante non plus, nous ne traiterons pas non plus les datasets de l'archive Monash ci-dessous :\n","- **London smart meters**,\n","- **wind farms**,\n","- **solar power**,\n","- **wind power**\n","- **Kaggle web traffic daily**\n","- **solar 10 minutely**\n"],"metadata":{"id":"BFBbri32CUus"}},{"cell_type":"markdown","metadata":{"id":"HcAJpEb762hr"},"source":["## Mise en place"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58182,"status":"ok","timestamp":1736282808164,"user":{"displayName":"Eva Robillard","userId":"06491556004018134427"},"user_tz":-60},"id":"P4pcNP701nZx","outputId":"96a6101a-bb41-44dc-c84f-9e912ee9d372"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","PATH = \"/content/drive/My Drive/projet_time_series/\"\n","import os\n","os.chdir(PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ByclnImO68Bf"},"outputs":[],"source":["import os\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pickle\n","import math\n","\n","from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n","from sklearn.svm import SVR\n","from sklearn.ensemble import RandomForestRegressor\n","from xgboost import XGBRegressor"]},{"cell_type":"markdown","metadata":{"id":"wWBRi6aa05rT"},"source":["## Test de Rocket pour la regression\n"]},{"cell_type":"markdown","source":["## Utilities"],"metadata":{"id":"VnRyfPDVwGp-"}},{"cell_type":"markdown","metadata":{"id":"tc1hgsqq7gB-"},"source":["### Rocket algorithm\n","\n","\n","ROCKET attend en entrée plusieurs séries temporelles univariées sous la forme d'une matrice 2D. Il peut être étendu pour traiter plusieurs séries temporelles multivariées\n","\n","> Texte ci-dessous issu de Rocket paper : https://arxiv.org/pdf/1910.13051\n","\n","ROCKET transforms time series using convolutional kernels as found in typical convolutional neural networks. The parameters of these kernels are randomly selected as follows:\n","\n","- **Length:** The length of the kernel is randomly selected from $\\{7, 9, 11\\}$ with equal probability, ensuring kernels are significantly shorter than input time series in most cases.\n","    \n","- **Weights:** The weights $w \\in W$ are sampled from a normal distribution $w \\sim \\mathcal{N}(0, 1)$ and are mean-centered: $\\omega = W - \\bar{W}$. This ensures that most weights remain relatively small, though larger magnitudes are possible.\n","    \n","- **Bias:** The bias $b$ is sampled from a uniform distribution $b \\sim \\mathcal{U}(-1, 1)$. Only positive values are used in the feature maps. The bias effectively shifts the feature maps above or below zero, emphasizing different aspects of the input data.\n","    \n","- **Dilation:** Dilation is sampled exponentially as $d = 2^x$, where $x \\sim \\mathcal{U}(0, A)$ and $A = \\log_2\\left(\\frac{l_{\\text{input}} - 1}{l_{\\text{kernel}} - 1}\\right)$. This ensures the effective kernel length matches the input time series while allowing kernels to capture patterns at various scales.\n","\n","- **Padding:** Padding is randomly applied with equal probability. If padding is used, zero-padding is added to the start and end of the series so that the kernel is centered at every point. Without padding, kernels focus on central patterns and may miss patterns at the edges.\n","\n","**Stride** is always set to one. Nonlinearities such as **ReLU are not applied**, ensuring the resulting feature maps are agnostic to specific transformations. Input time series are assumed to be **normalized** with a mean of zero and a standard deviation of one.\n","\n","These kernel parameters were determined to optimize classification accuracy on development datasets and generalize well to unseen data.\n","\n","\n","> Code ci-dessous issu de : https://github.com/angus924/rocket/blob/master/code/rocket_functions.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNNtCG-B7Ie8"},"outputs":[],"source":["# ROCKET ALGORITHM\n","###############################\n","# Angus Dempster, Francois Petitjean, Geoff Webb\n","#\n","# @article{dempster_etal_2020,\n","#   author  = {Dempster, Angus and Petitjean, Fran\\c{c}ois and Webb, Geoffrey I},\n","#   title   = {ROCKET: Exceptionally fast and accurate time classification using random convolutional kernels},\n","#   year    = {2020},\n","#   journal = {Data Mining and Knowledge Discovery},\n","#   doi     = {https://doi.org/10.1007/s10618-020-00701-z}\n","# }\n","#\n","# https://arxiv.org/abs/1910.13051 (preprint)\n","\n","import numpy as np\n","from numba import njit, prange\n","\n","@njit(\"Tuple((float64[:],int32[:],float64[:],int32[:],int32[:]))(int64,int64)\")\n","def generate_kernels(input_length, num_kernels):\n","    \"\"\"\n","    Generates random convolutional kernels as described in the ROCKET paper\n","\n","    Parameters:\n","    - input_length: Length of the input time series\n","    - num_kernels: Number of random kernels to generate\n","\n","    Returns:\n","    - weights: Flattened array of kernel weights\n","    - lengths: Array of kernel lengths (randomly chosen from {7, 9, 11})\n","    - biases: Array of biases (random values in [-1, 1])\n","    - dilations: Array of dilation values (sampled on an exponential scale)\n","    - paddings: Array of padding values (randomly applied)\n","    \"\"\"\n","\n","    # Candidate kernel lengths (ROCKET uses lengths {7, 9, 11})\n","    candidate_lengths = np.array((7, 9, 11), dtype = np.int32)\n","    lengths = np.random.choice(candidate_lengths, num_kernels)\n","\n","    # Initialize arrays for kernel parameters\n","    weights = np.zeros(lengths.sum(), dtype = np.float64)  # Kernel weights\n","    biases = np.zeros(num_kernels, dtype = np.float64)     # Kernel biases\n","    dilations = np.zeros(num_kernels, dtype = np.int32)    # Kernel dilations\n","    paddings = np.zeros(num_kernels, dtype = np.int32)     # Kernel paddings\n","\n","    a1 = 0  # Pointer for the start of each kernel's weights\n","\n","    for i in range(num_kernels):\n","\n","        _length = lengths[i]  # Length of the current kernel\n","\n","        # Generate weights from a normal distribution (mean 0, std 1)\n","        _weights = np.random.normal(0, 1, _length)\n","\n","        b1 = a1 + _length\n","        weights[a1:b1] = _weights - _weights.mean()  # Center the weights (mean = 0)\n","\n","        # Generate bias from a uniform distribution in [-1, 1]\n","        biases[i] = np.random.uniform(-1, 1)\n","\n","        # Compute dilation on an exponential scale\n","        dilation = 2 ** np.random.uniform(0, np.log2((input_length - 1) / (_length - 1)))\n","        dilation = np.int32(dilation)\n","        dilations[i] = dilation\n","\n","        # Randomly apply padding (0 or centered padding)\n","        padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0\n","        paddings[i] = padding\n","\n","        a1 = b1  # Move pointer to the next kernel's weights\n","\n","    return weights, lengths, biases, dilations, paddings\n","\n","@njit(fastmath = True)\n","def apply_kernel(X, weights, length, bias, dilation, padding):\n","    \"\"\"\n","    Applies a single convolutional kernel to a time series\n","\n","    In the code, X represents the input time series, but it is treated as a\n","    2D matrix, which means it can include univariate or multivariate time series,\n","    depending on its structure.\n","\n","    Parameters:\n","    - X: The input time series of size\n","    - weights: Weights of the kernel\n","    - length: Length of the kernel\n","    - bias: Bias of the kernel\n","    - dilation: Dilation factor\n","    - padding: Padding size\n","\n","    Returns:\n","    - ppv: Proportion of positive values in the feature map\n","    - max: Maximum value in the feature map\n","    \"\"\"\n","\n","    input_length = len(X)\n","\n","    # Compute the output length of the feature map\n","    output_length = (input_length + (2 * padding)) - ((length - 1) * dilation)\n","\n","    _ppv = 0  # Proportion of positive values\n","    _max = np.NINF  # Maximum value in the feature map (initialized to negative infinity)\n","\n","    # Define the range for convolution\n","    end = (input_length + padding) - ((length - 1) * dilation)\n","\n","    for i in range(-padding, end):  # Iterate over the time series with padding\n","\n","        _sum = bias  # Initialize convolution result with the bias\n","\n","        index = i\n","\n","        for j in range(length):  # Apply the kernel\n","\n","            if index > -1 and index < input_length:  # Check if the index is within bounds\n","                _sum = _sum + weights[j] * X[index]  # Convolve with weights\n","\n","            index = index + dilation  # Apply dilation\n","\n","        if _sum > _max:  # Update maximum value\n","            _max = _sum\n","\n","        if _sum > 0:  # Count positive values for ppv\n","            _ppv += 1\n","\n","    # Return the proportion of positive values and the maximum\n","    return _ppv / output_length, _max\n","\n","\n","@njit(\"float64[:,:](float64[:,:],Tuple((float64[::1],int32[:],float64[:],int32[:],int32[:])))\", parallel = True, fastmath = True)\n","def apply_kernels(X, kernels):\n","    \"\"\"\n","    Applies all generated kernels to a batch of time series\n","\n","    In the code, X represents the input time series, but it is treated as a\n","    2D matrix, which means it can include univariate or multivariate time series,\n","    depending on its structure.\n","\n","    Parameters:\n","    - X: A batch of input time series (2D array)\n","    - kernels: Tuple containing kernel parameters\n","\n","    Returns:\n","    - _X: A transformed feature matrix (2 features per kernel: ppv and max)\n","   \"\"\"\n","\n","    weights, lengths, biases, dilations, paddings = kernels\n","\n","    num_examples, _ = X.shape\n","    num_kernels = len(lengths)\n","\n","    # Initialize the output feature matrix\n","    _X = np.zeros((num_examples, num_kernels * 2), dtype = np.float64)  # Two features per kernel: ppv and max\n","\n","    for i in prange(num_examples):  # Parallelize over examples\n","\n","        a1 = 0  # Pointer for weights\n","        a2 = 0  # Pointer for features\n","\n","        for j in range(num_kernels):\n","\n","            b1 = a1 + lengths[j]\n","            b2 = a2 + 2\n","\n","            # Apply a single kernel and store the resulting features\n","            _X[i, a2:b2] = \\\n","            apply_kernel(X[i], weights[a1:b1], lengths[j], biases[j], dilations[j], paddings[j])\n","\n","            a1 = b1  # Move pointer to the next kernel's weights\n","            a2 = b2  # Move pointer to the next feature set\n","\n","    return _X\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FiRUy_bAPSys"},"source":["**Forme de X :**\n","\n","$X$ est une matrice 2D où :\n","\n","- Les lignes correspondent aux exemples ou au nombre de séries temporelles dans le dataset (num_examples). Chaque exemple correspond à une série temporelle différente.\n","\n","- Les colonnes correspondent aux points temporels (input_length)\n","\n","**ST univariés :** $X$ doit avoir la forme :\n","\n","`X.shape = (num_examples, input_length)`\n","\n","**ST multivariées :** $X$ doit avoir la forme :\n","\n","`X.shape = (num_examples, num_variables, input_length)`\n","\n","> Actuellement Rocket ne supporte pas les Séries Temporelles Multivariées, mais il est possible de l'adapter pour ce cas."]},{"cell_type":"markdown","source":["---\n","\n","**Comment adapter ROCKET au multivarié**\n","\n","Il est possible d'adapter l'algorithme ROCKET pour les séries temporelles multivariées. Pour ce faire, nous pouvons utiliser la méthode décrite dans le papier MiniROCKET (https://arxiv.org/pdf/2012.08791) qui est une amélioration de ROCKET. Cette méthode consiste à appliquer les mêmes noyaux de convolution indépendamment à chaque variable d'une série multivariée.\n","\n","Cette approche permet d'extraire les caractéristiques de chaque variable individuellement, sans essayer d'exploiter directement les corrélations entre les variables.\n","Cette indépendance facilite l'application sur des Séries Temporelles Multivariées, en particulier lorsque les variables sont hétérogènes (par exemple, des capteurs avec des unités ou des dynamiques différentes). \\\n","\n","Nous avons donc implémenté cette approche avec la fonction `apply_kernels_multivariate`.\n","\n","Cependant, cette approche entraîne une augmentation linéaire du nombre de features calculées par ROCKET :\n","\n","`Taille totale des features  = num_kernels * 2 * num_variables`\n","\n","Et ceci devient ingérable en mémoire pour de grands datasets.\n","\n","Comme cette approche multivariée de ROCKET applique les mêmes noyaux de convolution de façon indépendante sur chaque variable, une approche équivalente à laquelle nous avons pensé est d'appliquer ROCKET de manière totalement indépendante sur chaque variable univariée de la série temporelle multivariée. Cela revient exactement à ce que fait ROCKET multivarié, mais avec des avantages en termes de mémoire et de modularité.\n","\n","Pour ce faire, nous suivrons les étapes suivantes:\n","\n","Pour les séries temporelles **univariées**\n","- Générer des noyaux : Les noyaux sont générés pour chaque série univariée (avec generate_kernels).\n","- Appliquer ROCKET univarié pour chaque série univariée. Notons que les noyaux appliqués pour chaque série univariée sont différents d'une série à l'autre.\n","\n","Pour les séries temporelles **multivariées**\n","- Générer une fois les noyaux : Les noyaux sont générés une seule fois (avec generate_kernels).\n","- Appliquer ROCKET univarié pour chaque variable : on applique les mêmes noyaux précedemment générés indépendamment à chaque variable de la série temporelle multivariée.\n","- Concaténer les résultats : Les features générées pour chaque variable sont concaténées pour obtenir un tableau final.\n","\n","\n","Enfin, notons que l'approche qui consiste à appliquer les mêmes noyaux indépendamment sur chaque variable d'une série temporelle multivariée ne permet pas de capturer les dépendances entre les variables. Pour palier ce problème, l'algorithme **MultiROCKET** a été développé (https://arxiv.org/abs/2102.00457). Nous ne sommes pas sûres d'avoir le temps de tester MultiRocket et si nous avons le temps, nous pourrions églement avoir des problèmes de d'explosion mémoire pour de grosses séries temporelles multivariées.\n"],"metadata":{"id":"wWgLfqekoTS2"}},{"cell_type":"markdown","source":["### Chargement archive Monash"],"metadata":{"id":"Rq5kRPTSxNkt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AcH-mZSfkY4m"},"outputs":[],"source":["# Chargement des datasets de l'archive Monash\n","# Fonction issue de https://github.com/rakshitha123/TSForecasting?tab=readme-ov-file\n","######################################\n","from datetime import datetime\n","from distutils.util import strtobool\n","import pandas as pd\n","\n","# Converts the contents in a .tsf file into a dataframe and returns it along with other meta-data of the dataset: frequency, horizon, whether the dataset contains missing values and whether the series have equal lengths\n","#\n","# Parameters\n","# full_file_path_and_name - complete .tsf file path\n","# replace_missing_vals_with - a term to indicate the missing values in series in the returning dataframe\n","# value_column_name - Any name that is preferred to have as the name of the column containing series values in the returning dataframe\n","def convert_tsf_to_dataframe(\n","    full_file_path_and_name,\n","    replace_missing_vals_with=\"NaN\",\n","    value_column_name=\"series_value\",\n","):\n","    col_names = []\n","    col_types = []\n","    all_data = {}\n","    line_count = 0\n","    frequency = None\n","    forecast_horizon = None\n","    contain_missing_values = None\n","    contain_equal_length = None\n","    found_data_tag = False\n","    found_data_section = False\n","    started_reading_data_section = False\n","\n","    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n","        for line in file:\n","            # Strip white space from start/end of line\n","            line = line.strip()\n","\n","            if line:\n","                if line.startswith(\"@\"):  # Read meta-data\n","                    if not line.startswith(\"@data\"):\n","                        line_content = line.split(\" \")\n","                        if line.startswith(\"@attribute\"):\n","                            if (\n","                                len(line_content) != 3\n","                            ):  # Attributes have both name and type\n","                                raise Exception(\"Invalid meta-data specification.\")\n","\n","                            col_names.append(line_content[1])\n","                            col_types.append(line_content[2])\n","                        else:\n","                            if (\n","                                len(line_content) != 2\n","                            ):  # Other meta-data have only values\n","                                raise Exception(\"Invalid meta-data specification.\")\n","\n","                            if line.startswith(\"@frequency\"):\n","                                frequency = line_content[1]\n","                            elif line.startswith(\"@horizon\"):\n","                                forecast_horizon = int(line_content[1])\n","                            elif line.startswith(\"@missing\"):\n","                                contain_missing_values = bool(\n","                                    strtobool(line_content[1])\n","                                )\n","                            elif line.startswith(\"@equallength\"):\n","                                contain_equal_length = bool(strtobool(line_content[1]))\n","\n","                    else:\n","                        if len(col_names) == 0:\n","                            raise Exception(\n","                                \"Missing attribute section. Attribute section must come before data.\"\n","                            )\n","\n","                        found_data_tag = True\n","                elif not line.startswith(\"#\"):\n","                    if len(col_names) == 0:\n","                        raise Exception(\n","                            \"Missing attribute section. Attribute section must come before data.\"\n","                        )\n","                    elif not found_data_tag:\n","                        raise Exception(\"Missing @data tag.\")\n","                    else:\n","                        if not started_reading_data_section:\n","                            started_reading_data_section = True\n","                            found_data_section = True\n","                            all_series = []\n","\n","                            for col in col_names:\n","                                all_data[col] = []\n","\n","                        full_info = line.split(\":\")\n","\n","                        if len(full_info) != (len(col_names) + 1):\n","                            raise Exception(\"Missing attributes/values in series.\")\n","\n","                        series = full_info[len(full_info) - 1]\n","                        series = series.split(\",\")\n","\n","                        if len(series) == 0:\n","                            raise Exception(\n","                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n","                            )\n","\n","                        numeric_series = []\n","\n","                        for val in series:\n","                            if val == \"?\":\n","                                numeric_series.append(replace_missing_vals_with)\n","                            else:\n","                                numeric_series.append(float(val))\n","\n","                        if numeric_series.count(replace_missing_vals_with) == len(\n","                            numeric_series\n","                        ):\n","                            raise Exception(\n","                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n","                            )\n","\n","                        all_series.append(pd.Series(numeric_series).array)\n","\n","                        for i in range(len(col_names)):\n","                            att_val = None\n","                            if col_types[i] == \"numeric\":\n","                                att_val = int(full_info[i])\n","                            elif col_types[i] == \"string\":\n","                                att_val = str(full_info[i])\n","                            elif col_types[i] == \"date\":\n","                                att_val = datetime.strptime(\n","                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n","                                )\n","                            else:\n","                                raise Exception(\n","                                    \"Invalid attribute type.\"\n","                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n","\n","                            if att_val is None:\n","                                raise Exception(\"Invalid attribute value.\")\n","                            else:\n","                                all_data[col_names[i]].append(att_val)\n","\n","                line_count = line_count + 1\n","\n","        if line_count == 0:\n","            raise Exception(\"Empty file.\")\n","        if len(col_names) == 0:\n","            raise Exception(\"Missing attribute section.\")\n","        if not found_data_section:\n","            raise Exception(\"Missing series information under data section.\")\n","\n","        all_data[value_column_name] = all_series\n","        loaded_data = pd.DataFrame(all_data)\n","\n","        return (\n","            loaded_data,\n","            frequency,\n","            forecast_horizon,\n","            contain_missing_values,\n","            contain_equal_length,\n","        )\n","\n","# Example of usage\n","# loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"TSForecasting/tsf_data/sample.tsf\")\n","\n","# print(loaded_data)\n","# print(frequency)\n","# print(forecast_horizon)\n","# print(contain_missing_values)\n","# print(contain_equal_length)"]},{"cell_type":"markdown","source":["**Précisions sur l'archive Monash**\n","\n","Dans l'archive Monash, les séries temporelles sont sous la forme suivante :\n","\n","**Séries Univariées**\n","- Chaque fichier représente **plusieurs** séries tremporelles univariées.\n","  - Chaque ligne représente une série temporelle univariée\n","  - Les colonnes représentent un pas de temps.\n","\n","**Séries Multivariées**\n","- Chaque fichier représente **une** série temporelle multivariée.\n","  - Lignes $T$ : Chaque ligne correspond à un pas de temps unique.\n","  - Colonnes $n$ : Chaque colonne correspond à une variable."],"metadata":{"id":"m9ggeq4caeNo"}},{"cell_type":"markdown","source":["### Déterminer le Forecast Horizon"],"metadata":{"id":"LH06LIndxUp1"}},{"cell_type":"markdown","metadata":{"id":"8qgdkpL8Hof1"},"source":["**Dans le papier du Monash dataset :**\n","\n","Biblio pour cette fontion :\n","-  papier Monsah : https://openreview.net/pdf?id=wEc1mgAjU-\n","\n","**Extrait du papier Monash (https://openreview.net/pdf?id=wEc1mgAjU-)** :\n","\n","\"Forecast horizons are chosen for each dataset to evaluate the model performance. For all competition datasets, we use the forecast horizons originally employed in the competitions. For the remaining datasets, 12 months ahead forecasts are obtained for monthly datasets, 8 weeks ahead forecasts are obtained for weekly datasets, except the solar weekly dataset, and 30 days ahead forecasts are obtained for daily datasets. For the solar weekly dataset, we use a horizon of 5 as the series in this dataset are relatively short compared with other weekly datasets. For half-hourly, hourly and other high-frequency datasets, we set the forecasting horizon to one week, e.g., 168 is used as the horizon for hourly datasets.\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7sVP16dHn5h"},"outputs":[],"source":["def determine_forecast_horizon(frequency, dataset_name, forecast_horizon):\n","    \"\"\"\n","    Détermine l'horizon temporel (forecast horizon) selon l'heuristique décrite pour les datasets du Monash Dataset.\n","\n","    From the Monash Time Series Forecasting Archive Paper (https://openreview.net/pdf?id=wEc1mgAjU-):\n","    >>> Forecast horizons are chosen for each dataset to evaluate the model performance. For all competition datasets,\n","    we use the forecast horizons originally employed in the competitions. For the remaining datasets, 12 months ahead\n","    forecasts are obtained for monthly datasets, 8 weeks ahead forecasts are obtained for weekly datasets,\n","    except the solar weekly dataset, and 30 days ahead forecasts are obtained for daily datasets. For the solar\n","     weekly dataset, we use a horizon of 5 as the series in this dataset are relatively short compared with other\n","     weekly datasets. For half-hourly, hourly and other high-frequency datasets, we set the forecasting horizon\n","     to one week, e.g., 168 is used as the horizon for hourly datasets. <<<\n","\n","    Parameters:\n","        frequency (str): Fréquence de la série temporelle (e.g., 'monthly', 'weekly', 'daily', 'hourly').\n","        dataset_name (str): Nom du dataset, utilisé pour des cas spécifiques (e.g., 'solar weekly').\n","        forecast_horizon (int): Horizon temporel fourni directement. Si None, l'horizon est déterminé automatiquement.\n","\n","    Returns:\n","        int: L'horizon temporel pour le dataset donné.\n","    \"\"\"\n","    # Si l'horizon est déjà fourni, on le retourne directement\n","    if forecast_horizon is not None:\n","        return forecast_horizon\n","\n","    # Heuristiques selon la fréquence\n","    frequency_to_horizon = {\n","        'yearly' : 1,        # 1 an\n","        'quaterly' : 4,      # 4 trimestres (1 an)\n","        'monthly': 12,       # 12 mois (1 an)\n","        'weekly': 8,         # 8 semaines\n","        'daily': 30,         # 30 jours\n","        'hourly': 168,       # 1 semaine = 168 heures\n","        'half_hourly': 336,  # 1 semaine = 336 demi-heures\n","        '10-minutely': 1008, # 1 semaine = 1008 intervalles de 10 minutes\n","    }\n","\n","    # Exception pour le dataset \"solar weekly\"\n","    if \"solar_weekly\" in dataset_name.lower():\n","        return 5\n","\n","    # Vérifier si la fréquence est dans les règles\n","    if frequency in frequency_to_horizon:\n","        return frequency_to_horizon[frequency]\n","\n","    # Si la fréquence n'est pas reconnue\n","    raise ValueError(f\"Fréquence inconnue ou non supportée : {frequency}\")"]},{"cell_type":"markdown","source":["### Déterminer les lags (= taille fenêtre glissante)"],"metadata":{"id":"uid8JezJxeDq"}},{"cell_type":"markdown","metadata":{"id":"FA7wtv-IHcPs"},"source":["**Dans le papier du Monash dataset** :\n","\n","Biblio pour cette fontion :\n","-  https://openreview.net/pdf?id=wEc1mgAjU-\n","-  https://openreview.net/pdf?id=wEc1mgAjU-\n","\n","Les valeurs retardées (ou \"lags\") correspondent aux observations passées utilisées par le modèle pour faire des prédictions futures.\n","\n","Donc :  lag = k = taille de la fenêtre glissante\n","\n","**Extrait du papier Monash (https://openreview.net/pdf?id=wEc1mgAjU-)**:\n","\n","The number of lagged values used in the global models are determined based on a heuristic suggested in prior work [59]. Generally, the number of lagged values is chosen as the seasonality multiplied with 1.25. If the datasets contain short series and it is impossible to use the above defined number of lags, for example in the Dominick and solar weekly datasets, then the number of lagged values is chosen as the forecast horizon multiplied with 1.25, assuming that the horizon is not arbitrarily chosen but based on certain characteristics of the time series structure. When defining the number of lagged values for multi-seasonal datasets, we consider the corresponding weekly seasonality value, e.g., 168 for hourly datasets. If it is impossible to use the number of lagged values obtained with the weekly seasonality due to high memory and computational requirements, for example with the traffic hourly and electricity hourly datasets, then we use the corresponding daily seasonality value to define the number of lags, e.g., 24 for hourly datasets. In particular, due to high memory and computational requirements, the number of lagged values is chosen as 50 for the solar 10 minutely dataset which is less than the above mentioned heuristics based on seasonality and forecasting horizon suggest.\n","\n","\n","**Extrait du papier : https://arxiv.org/pdf/1909.00590** :\n","\n","Putting these ideas together, we select the input window size m with two options. One is to make the input window size slightly bigger than the output window size (m = 1.25 ∗ output window size). The other is to make the input window size slightly bigger than the seasonality period (m = 1.25 ∗ seasonality period). For instance if the time series has weekly seasonality (seasonality period = 7) with expected forecasting horizon 56, with the first option we set the input window size to be 70 (1.25 ∗ 56), whereas with the second option it is set to be 9 (1.25 ∗ 7). The constant 1.25 is selected purely as a heuristic. The idea is to ascertain that every recurrent cell instance at each time step gets to see at least its last periodic cycle so that it can model any remaining stochastic seasonality. In case, the total length of the time series is too short, m is selected to be either of the two feasible, or some other possible value if none of them work. For instance, the forecasting horizon 6 subgroup of the CIF 2016 dataset comprises of such short series and m is selected to be 7 (slightly bigger than the output window size) even though the seasonality period is equal to 12.\n"]},{"cell_type":"code","source":["def determine_lag(frequency, forecast_horizon, dataset_name, series_length, multiply_by=1):\n","    \"\"\"\n","    Détermine le nombre de lags selon l'heuristique décrite dans le papier du Monash Dataset.\n","    Si le lag calculé est plus petit que l'horizon de prévision, il est ajusté à forecast_horizon.\n","    Les lags correspondent aux observations passées utilisées par le modèle pour faire des prédictions futures.\n","    Si on a une série temporelle Yt​, les lagged values sont des décalages temporels de cette série :\n","    Y_{t−1}, Y_{t−2},...,Y_{t−p}​, où p est le nombre de lagged values.\n","\n","    La taille de la fenêtre glissante peut être fixée égale au nombre de lagged values :\n","    Window Size = Number of Lagged Values\n","    Cela garantit que chaque fenêtre contiendra suffisamment d'informations pour que\n","    le modèle capture les motifs temporels pertinents.\n","\n","    From the Monash Time Series Forecasting Archive Paper (https://openreview.net/pdf?id=wEc1mgAjU-):\n","    >>> The number of lagged values used in the global models are determined based on a heuristic suggested in\n","    prior work [59]. Generally, the number of lagged values is chosen as the seasonality multiplied with 1.25.\n","    If the datasets contain short series and it is impossible to use the above defined number of lags, for\n","     example in the Dominick and solar weekly datasets, then the number of lagged values is chosen as the\n","     forecast horizon multiplied with 1.25, assuming that the horizon is not arbitrarily chosen but based on\n","     certain characteristics of the time series structure. When defining the number of lagged values for\n","     multi-seasonal datasets, we consider the corresponding weekly seasonality value, e.g., 168 for hourly\n","     datasets. If it is impossible to use the number of lagged values obtained with the weekly seasonality\n","     due to high memory and computational requirements, for example with the traffic hourly and electricity\n","     hourly datasets, then we use the corresponding daily seasonality value to define the number of lags,\n","    e.g., 24 for hourly datasets. In particular, due to high memory and computational requirements, the\n","    number of lagged values is chosen as 50 for the solar 10 minutely dataset which is less than the above\n","    mentioned heuristics based on seasonality and forecasting horizon suggest. <<<\n","\n","    Parameters:\n","        frequency (str): Fréquence de la série temporelle (e.g., 'monthly', 'weekly', 'daily', 'hourly').\n","        forecast_horizon (int): Horizon de prévision.\n","        dataset_name (str, optional): Nom du dataset, utilisé pour des cas spécifiques (e.g., 'solar weekly').\n","        series_length (int, optional): Longueur totale de la série. Nécessaire pour vérifier les contraintes.\n","        multiply_by (int, optional): Facteur de multiplication du lag.\n","\n","    Returns:\n","        lag (int): Le lag pour le dataset donné.\n","        seasonality (int) : saisonnalité du dataset donné.\n","    \"\"\"\n","    # Saisonnalités définies par fréquence\n","    frequency_to_seasonality = {\n","        'yearly': 1,\n","        'quarterly': 4,\n","        'monthly': 12,\n","        'weekly': 7,\n","        'daily': 7,\n","        'hourly': 168,\n","        'half_hourly': 336,\n","        '10-minutely': 1008,\n","    }\n","\n","    # Exception pour des cas spécifiques\n","    if \"solar_weekly\" in dataset_name.lower():\n","        return math.ceil(5 * 1.25)  # Spécifique à ce dataset\n","\n","    # Obtenir la saisonnalité\n","    seasonality = frequency_to_seasonality.get(frequency, None)\n","    if seasonality is None:\n","        raise ValueError(f\"Fréquence inconnue ou non supportée : {frequency}\")\n","\n","    # Calculer les deux options pour le lag\n","    option1 = math.ceil(forecast_horizon * 1.25)\n","    option2 = math.ceil(seasonality * 1.25)\n","\n","    # Choisir la meilleure option selon la longueur de la série\n","    if series_length is not None and series_length < option2:\n","        lag = option1\n","    else:\n","        lag = option2\n","\n","    # Contraintes mémoire pour certains datasets spécifiques\n","    if \"traffic hourly\" in dataset_name.lower() or \"electricity hourly\" in dataset_name.lower():\n","        lag = min(lag, math.ceil(24 * 1.25))\n","    if \"10-minutely\" in dataset_name.lower():\n","        lag = 50  # Fixe pour ce dataset\n","\n","    # Ajuster pour garantir m >= forecast_horizon\n","    lag = max(lag, forecast_horizon)\n","\n","    return lag, seasonality"],"metadata":{"id":"47YhbfHHJpcc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Mise en forme du jeu de données"],"metadata":{"id":"PSH9dpsLxpg_"}},{"cell_type":"markdown","source":["**Mettre en forme le jeu de données**\n","\n","Si data est univarié alors data[\"series_value\"] est de la forme : (num_examples, time_length)\n","c'est à dire composé de plusieurs ST univariées. Chaque ligne correspond à une ST univariée\n","et chaque colonne a un pas de temps.\n","\n","Si data est multivarié alors data[\"series_value\"] est de la forme : (time_length, num_variables)\n","c'est à dire composé UNE seule ST multivariée. Chaque ligne correspond à un pas de temps et\n","et chaque colonne a une variable. Afin d'appliquer les mêmes noyaux de ROCKET indépendament\n","sur chaque variable, on transpose data[\"series_value\"] pour qu'il soit de la forme\n","(num_variables, time_length). Ainsi, une ST multivariée est vue comme plusieurs ST univariées.\n","On ne prends pas en compte ici la dépendace entre les variables."],"metadata":{"id":"5Wi3-lZq1upq"}},{"cell_type":"code","source":["def preprocess_series(data, dataset_name):\n","    \"\"\"\n","    Crée le jeu de données contenant les séries tremporelles preprocessed.\n","    Préprocesser les séries temporelles selon qu'elles soient univariées ou multivariées\n","    avec normalisation des données et gestion des longueurs variable.\n","    Chaque ST univariée est normalisée indépendamment.\n","    De même, chaque variable de la ST multivariée est normalisée indépendamment.\n","\n","    Parameters:\n","        data (pandas.DataFrame): DataFrame contenant une colonne \"series_value\" qui stocke les séries temporelles.\n","            - Si univarié : \"series_value\" est de la forme (num_examples, time_length) (longueurs variables possibles).\n","            - Si multivarié : \"series_value\" est de la forme (time_length, num_variables) (longueurs variables possibles).\n","        dataset_name (str): Nom du dataset, utilisé pour déterminer s'il est univarié ou multivarié.\n","\n","    Returns:\n","        list of np.ndarray ou np.ndarray:\n","            - Si univarié : Liste de tableaux NumPy normalisés pour chaque série (indépendamment).\n","              Chaque tableau peut avoir une longueur différente.\n","              Chaque élément de la liste contient une ST univariée.\n","            - Si multivarié : Tableau NumPy normalisé (num_variables, time_length),\n","              chaque variable étant normalisée indépendamment.\n","              Chaque élément de la liste contient une variable de la ST multivariée.\n","    \"\"\"\n","    series_list = data[\"series_value\"].to_list()\n","\n","    if 'univariate' in dataset_name:\n","        # Données univariées : Normalisation série par série\n","        processed_series = [ # ST univariée : (num_examples, time_length)\n","            StandardScaler().fit_transform(np.array(serie).reshape(-1, 1)).flatten()\n","            for serie in series_list\n","        ]\n","        return np.array(processed_series, dtype=object)  # Retourne une liste de séries normalisées (longueurs variables possibles)\n","\n","    elif 'multivariate' in dataset_name:\n","        # Données multivariées\n","        mts = np.array(series_list, dtype=object)  # Tableau des séries multivariées\n","\n","        # Vérifier si les longueurs des séries sont homogènes\n","        if all(len(serie) == len(mts[0]) for serie in mts):\n","            # Transposer directement le tableau entier si toutes les longueurs sont homogènes\n","            mts = np.array(series_list).T  # Transpose de (time_length, num_variables) à (num_variables, time_length)\n","            scaler = StandardScaler()\n","            processed_series = np.array([scaler.fit_transform(var.reshape(-1, 1)).flatten() for var in mts])\n","            return processed_series\n","        else:\n","            # Si les longueurs sont inhomogènes, traiter chaque série séparément\n","            processed_series = []\n","            for serie in mts:\n","                transposed = np.array(serie).T  # Transposer la série multivariée\n","                processed_series.append(\n","                    [StandardScaler().fit_transform(var.reshape(-1, 1)).flatten() for var in transposed]\n","                )\n","            return processed_series  # Retourne une liste de séries normalisées\n","\n","    else:\n","        raise ValueError(\"Le type de dataset doit être spécifié dans le nom : 'univariate' ou 'multivariate'.\")\n"],"metadata":{"id":"SKCxtzRp3A_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Vérifier la taille de la série et adapter la taille de fenêtre en fonction"],"metadata":{"id":"2H2_wmow10eF"}},{"cell_type":"code","source":["def check_and_adjust_series(series, window_size, forecast_horizon, stride, min_window_size, min_stride=1):\n","    \"\"\"\n","    Combine la vérification de la taille de l'ensemble d'entraînement avec l'ajustement dynamique des paramètres de fenêtres glissantes.\n","\n","    Parameters:\n","        series (list or np.array): La série temporelle à analyser.\n","        window_size (int): Taille initiale de la fenêtre glissante.\n","        forecast_horizon (int): Horizon de prédiction.\n","        stride (int): Pas entre les fenêtres glissantes.\n","        min_window_size (int): Taille minimale de la fenêtre glissante.\n","        min_stride (int): Stride minimal autorisé.\n","\n","    Returns:\n","        dict: Contient les résultats :\n","            - 'is_valid' (bool): True si les paramètres sont valides.\n","            - 'adjusted_window_size' (int): Taille de fenêtre ajustée.\n","            - 'adjusted_stride' (int): Stride ajusté.\n","            - 'n_windows' (int): Nombre de fenêtres glissantes possibles.\n","            - 'min_required_length' (int): Longueur minimale requise pour l'ensemble d'entraînement.\n","            - 'reason' (str): Raison si la série est invalide.\n","    \"\"\"\n","    series_length = len(series)\n","\n","    # Ajuster la taille de la fenêtre et le stride\n","    adjusted_stride = max(min_stride, stride)\n","    adjusted_window_size = max(min_window_size, min(window_size, series_length // 2))\n","\n","    # Taille minimale requise pour générer des fenêtres valides\n","    min_required_length = adjusted_window_size + forecast_horizon + adjusted_stride - 1\n","\n","    # Taille de l'ensemble d'entraînement\n","    T_train = series_length - forecast_horizon\n","\n","    # Vérification si l'ensemble d'entraînement est suffisamment long\n","    if T_train < min_required_length:\n","        return {\n","            \"is_valid\": False,\n","            \"adjusted_window_size\": adjusted_window_size,\n","            \"adjusted_stride\": adjusted_stride,\n","            \"n_windows\": 0,\n","            \"min_required_length\": min_required_length,\n","            \"reason\": f\"Training set too short: T_train={T_train} < min_required_length={min_required_length}.\"\n","        }\n","\n","    # Calculer le nombre de fenêtres glissantes possibles\n","    n_windows = max(0, (series_length - adjusted_window_size - forecast_horizon) // adjusted_stride + 1)\n","\n","    # Vérification de validité finale\n","    if n_windows < 2:\n","        return {\n","            \"is_valid\": False,\n","            \"adjusted_window_size\": adjusted_window_size,\n","            \"adjusted_stride\": adjusted_stride,\n","            \"n_windows\": n_windows,\n","            \"min_required_length\": min_required_length,\n","            \"reason\": f\"Not enough windows (n_windows={n_windows} < 2).\"\n","        }\n","\n","    # Si tout est valide, retourner les paramètres ajustés\n","    return {\n","        \"is_valid\": True,\n","        \"adjusted_window_size\": adjusted_window_size,\n","        \"adjusted_stride\": adjusted_stride,\n","        \"n_windows\": n_windows,\n","        \"min_required_length\": min_required_length,\n","        \"reason\": None\n","    }"],"metadata":{"id":"DiGNDSmUD-zc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Division en train et test\n","\n","\n","Biblio pour cette fontion :\n","-  https://openreview.net/pdf?id=wEc1mgAjU-"],"metadata":{"id":"O_nm1Vy_x-DV"}},{"cell_type":"code","source":["def split_time_series(series, forecast_horizon):\n","    \"\"\"\n","    Divise une série temporelle en ensembles d'entraînement et de test.\n","\n","    Parameters:\n","        series (list or np.ndarray): Série temporelle à diviser.\n","        forecast_horizon (int): Taille de la partie forecast (H).\n","\n","    Returns:\n","        train (np.ndarray): Partie entraînement (l - H).\n","        test (np.ndarray): Partie validation (H).\n","    \"\"\"\n","    series = np.asarray(series)\n","    train = series[:-forecast_horizon]\n","    test = series[-forecast_horizon:]\n","    return train, test"],"metadata":{"id":"3v5AWjIxBqqo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Application des fenêtres glissantes"],"metadata":{"id":"iARjmb9LxxPD"}},{"cell_type":"markdown","metadata":{"id":"4PqYuTvSKT5K"},"source":["**Mise en forme des données pour le forecasting avec des fenêtres glissantes et un stride.**\n","\n","Biblio:\n","- https://github.com/timeseriesAI/tsai/blob/main/tsai/data/preparation.py\n","- https://towardsdatascience.com/fast-and-robust-sliding-window-vectorization-with-numpy-3ad950ed62f5\n","-  https://openreview.net/pdf?id=wEc1mgAjU-\n","\n"]},{"cell_type":"code","source":["def sliding_window(series, window_size, forecast_horizon, stride=1, fill_incomplete=True):\n","    \"\"\"\n","    Crée un jeu de données pour le forecasting avec des fenêtres glissantes et un stride.\n","\n","    Parameters:\n","        series (np.ndarray or list): Série temporelle univariée de forme (T,).\n","        window_size (int): Taille de la fenêtre d'entrée (W).\n","        forecast_horizon (int): Horizon de prévision (H).\n","        stride (int): Pas de décalage entre les fenêtres (par défaut : 1).\n","        fill_incomplete (bool): Si True, remplit les fenêtres de y incomplètes avec la dernière valeur disponible.\n","\n","    Returns:\n","        X (np.ndarray): Fenêtres d'entrée de forme (N, W).\n","        y (np.ndarray): Fenêtres de sortie de forme (N, H).\n","    \"\"\"\n","    # Convertir la série en tableau NumPy si ce n'est pas déjà fait\n","    series = np.asarray(series)\n","    T = len(series)  # Nombre de pas de temps (T)\n","\n","    # Vérifier que la fenêtre glissante est au moins aussi grande que l'horizon de prévision\n","    if window_size < forecast_horizon:\n","        raise ValueError(\"La taille de la fenêtre (window_size) ne peut pas être plus petite que l'horizon temporel (forecast_horizon).\")\n","\n","    # Calculer le nombre total de fenêtres\n","    n_windows = (T - window_size - forecast_horizon) // stride + 1\n","    if n_windows <= 0:\n","        raise ValueError(\"Taille de la série trop courte pour les paramètres donnés.\")\n","\n","    # Générer les indices pour les fenêtres d'entrée (X)\n","    input_indices = np.arange(window_size) + np.arange(0, n_windows * stride, stride)[:, None]\n","    input_indices = input_indices.astype(int)\n","\n","    X = series[input_indices]  # Fenêtres d'entrée : (N, W)\n","\n","    # Générer les indices pour les fenêtres de sortie (y)\n","    output_indices = np.arange(forecast_horizon) + np.arange(0, n_windows * stride, stride)[:, None] + window_size\n","\n","    # Traiter les indices de sortie incomplets si fill_incomplete est activé\n","    y = []\n","    for indices in output_indices:\n","        if indices[-1] < T:  # Cas où y est complet\n","            y.append(series[indices])\n","        else:  # Cas où y est incomplet\n","            incomplete_y = series[indices[indices < T]].tolist()\n","            last_value = incomplete_y[-1] if incomplete_y else series[-1]\n","            filled_y = incomplete_y + [last_value] * (forecast_horizon - len(incomplete_y))\n","            y.append(filled_y)\n","\n","    y = np.array(y)  # Fenêtres de sortie : (N, H)\n","\n","    return X, y"],"metadata":{"id":"RIGGJjKW7snU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reconstruire la série à partir des fenêtres glissante"],"metadata":{"id":"MZY0q6y2yJB7"}},{"cell_type":"code","source":["def inverse_sliding_window(X, y, window_size, forecast_horizon, stride=1):\n","    \"\"\"\n","    Reconstruit une série temporelle à partir des fenêtres glissantes.\n","\n","    Parameters:\n","        X (np.ndarray): Fenêtres d'entrée de forme (N, W).\n","        y (np.ndarray): Fenêtres de sortie de forme (N, H).\n","        window_size (int): Taille de la fenêtre d'entrée (W).\n","        forecast_horizon (int): Horizon de prévision (H).\n","        stride (int): Pas de décalage entre les fenêtres (par défaut : 1).\n","\n","    Returns:\n","        series_reconstructed (np.ndarray): Série temporelle reconstruite.\n","    \"\"\"\n","    # Dimensions des fenêtres\n","    n_windows, W = X.shape\n","    H = y.shape[1]\n","\n","    # Taille estimée de la série reconstruite\n","    total_length = (n_windows - 1) * stride + window_size + forecast_horizon\n","    series_reconstructed = np.zeros(total_length)\n","    weights = np.zeros(total_length)  # Poids pour moyenne des recouvrements\n","\n","    for i in range(n_windows):\n","        start_x = i * stride\n","        end_x = start_x + window_size\n","        series_reconstructed[start_x:end_x] += X[i]\n","        weights[start_x:end_x] += 1\n","\n","        start_y = end_x\n","        end_y = start_y + forecast_horizon\n","        series_reconstructed[start_y:end_y] += y[i]\n","        weights[start_y:end_y] += 1\n","\n","    # Normaliser par les poids pour moyenne dans les zones de recouvrement\n","    weights[weights == 0] = 1  # Éviter la division par zéro\n","    series_reconstructed /= weights\n","\n","    return series_reconstructed"],"metadata":{"id":"YTE7ILM4ZvF7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Entraîner un régresseur après ROCKET\n"],"metadata":{"id":"AVKnBY_WyDyi"}},{"cell_type":"code","source":["def train_and_predict_model(X_train_features, y_train, X_test_features, model_name='ridge', **kwargs):\n","    \"\"\"\n","    Entraîne un modèle de régression spécifié et effectue une prédiction.\n","\n","    Parameters:\n","        X_train_features (np.ndarray): Caractéristiques d'entraînement (features) issues de ROCKET.\n","        y_train (np.ndarray): Labels ou cibles d'entraînement.\n","        X_test_features (np.ndarray): Caractéristiques de test (features) issues de ROCKET.\n","        model_name (str): Le nom du modèle à utiliser. Options disponibles :\n","                          - 'ridge' pour RidgeCV (régression Ridge).\n","                          - 'lasso' pour LassoCV (régression Lasso).\n","                          - 'elasticnet' pour ElasticNetCV (régression ElasticNet).\n","                          - 'svr' pour SVR (Support Vector Regression).\n","                          - 'random_forest' pour RandomForestRegressor (régression Random Forest).\n","                          - 'xgboost' pour XGBRegressor (régression XGBoost).\n","        **kwargs: Paramètres supplémentaires spécifiques au modèle choisi.\n","\n","    Returns:\n","        tuple:\n","            - y_pred (np.ndarray): Prédictions effectuées par le modèle.\n","            - regressor (object): Instance du modèle entraîné.\n","\n","    Avantages et inconvénients des modèles :\n","    - RidgeCV: L2 regularization. Convient aux datasets avec de nombreuses features corrélées.\n","                Pas de sparsité dans les coefficients.\n","    - LassoCV: L1 regularization. Encourage la sparsité et sélectionne les features importantes.\n","                Peut ignorer certaines features pertinentes si elles sont corrélées.\n","    - ElasticNetCV: Combine L1 et L2. Bon compromis entre sélection de features et généralisation.\n","                     Convient si beaucoup de features sont corrélées.\n","    - SVR: Bonne performance avec des noyaux non linéaires comme RBF. Nécessite une mise à l'échelle des données.\n","           Peut être lent pour de grands datasets.\n","    - RandomForestRegressor: Manipule bien de nombreuses features. Robuste au surapprentissage.\n","                             Moins interprétable et plus coûteux en mémoire.\n","    - XGBRegressor: Performant même avec des déséquilibres ou de nombreuses features inutiles.\n","                    Nécessite un ajustement précis des hyperparamètres pour éviter le surapprentissage.\n","    \"\"\"\n","    if model_name == 'ridge':\n","        # Ridge Regression (L2 Regularization): Gère les features corrélées sans sparsité\n","        regressor = RidgeCV(alphas=np.logspace(-3, 3, 7), **kwargs)\n","        regressor.fit(X_train_features, y_train)\n","\n","    elif model_name == 'lasso':\n","        # Lasso Regression (L1 Regularization): Encourage la sparsité\n","        regressor = LassoCV(alphas=np.logspace(-3, 3, 7), cv=5, **kwargs)\n","        regressor.fit(X_train_features, y_train)\n","\n","    elif model_name == 'elasticnet':\n","        # ElasticNet (L1 + L2 Regularization): Combine sparsité et robustesse\n","        regressor = ElasticNetCV(l1_ratio=0.5, alphas=np.logspace(-3, 3, 7), cv=5, **kwargs)\n","        regressor.fit(X_train_features, y_train)\n","\n","    elif model_name == 'svr':\n","        # Support Vector Regression (SVR): Bon pour des relations complexes, mais lent sur de grands datasets\n","        regressor = SVR(kernel='linear', C=1.0, **kwargs)\n","        regressor.fit(X_train_features, y_train)\n","\n","    elif model_name == 'random_forest':\n","        # Random Forest: Gère de nombreuses features, robuste, mais moins interprétable\n","        regressor = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, **kwargs)\n","        regressor.fit(X_train_features, y_train)\n","\n","    elif model_name == 'xgboost':\n","        # XGBoost: Puissant et performant, mais nécessite un ajustement d'hyperparamètres\n","        regressor = XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=5, learning_rate=0.1, **kwargs)\n","        regressor.fit(X_train_features, y_train)\n","\n","    else:\n","        raise ValueError(f\"Modèle inconnu : {model_name}. Options disponibles : 'ridge', 'lasso', 'elasticnet', 'svr', 'random_forest', 'xgboost'.\")\n","\n","    # Effectuer les prédictions sur les données de test\n","    y_pred = regressor.predict(X_test_features)\n","\n","    return y_pred, regressor\n"],"metadata":{"id":"HpI1PP8u_US7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Calcul du MASE"],"metadata":{"id":"J3s2n-t4yPX8"}},{"cell_type":"markdown","source":["Le **modèle naïf** est une approche de prévision très simple, souvent utilisée comme référence ou benchmark minimal pour évaluer les performances d'autres modèles.\n","\n","*Principe* : La prochaine valeur de la série temporelle est égale à la dernière valeur observée.\n","On suppose que la série ne présente pas de tendance ni de saisonnalité significative.\n","Formule : $\\hat{y}_{t+h} = y_t$\n","\n","Le **modèle snaïve** (saisonnier naïf) est une extension du modèle naïf classique qui exploite la saisonnalité présente dans une série temporelle. Plutôt que de prédire uniquement en fonction de la dernière observation, il répète les valeurs des dernières périodes de saisonnalité.\n","\n","*Principe* : La série temporelle est saisonnière, avec une période connue ($s$). Les valeurs futures suivent le même motif que les valeurs des cycles précédents.\n","Formule : $\\hat{y}_{t+h} = y_{t+h-s}$"],"metadata":{"id":"Xnmrd5ZLTUZP"}},{"cell_type":"markdown","source":["Le **Mean Absolute Scaled Error (MASE)** est défini comme suit :\n","\n","$$\n","\\text{MASE} = \\frac{\\text{MAE du modèle}}{\\text{MAE du modèle naïf}}\n","$$\n","\n","- **MAE du modèle**: Moyenne des erreurs absolues entre les prédictions $\\hat{y}$ et les valeurs réelles $y$.\n","- **MAE du modèle naïf** : Moyenne des erreurs absolues pour le modèle naïf sur la série d'entraînement. Le modèle naïf utilise la dernière observation pour prédire la suivante :\n","$$\n","\\text{MAE naïf} = \\frac{1}{T-1} \\sum_{t=2}^{T} |y_t - y_{t-1}|\n","$$\n","\n","Interprétation :\n","- $MASE < 0.5 $ : Le modèle est significativement meilleur que le modèle naïf.\n","- $0.5 < MASE < 1$ : Le modèle est légèrement meilleur que le modèle naîf, mais l'amélioration est modérée.  \n","- $MASE = 1$ : Le modèle né apporte aucune amélioration par rapport au modèle naïf.\n","- $MASE > 1$ : Le modèle est moins performant que le modèle naïf."],"metadata":{"id":"32HLAk46Ui73"}},{"cell_type":"code","source":["def compute_mase(actual_values, predicted_values, training_series, seasonality, epsilon=1e-8):\n","    \"\"\"\n","    Calcul du Mean Absolute Scaled Error (MASE) avec gestion des séries courtes.\n","\n","    Parameters:\n","        actual_values (np.ndarray): Valeurs réelles (test) à prédire.\n","        predicted_values (np.ndarray): Valeurs prédites par le modèle.\n","        training_series (np.ndarray): Série temporelle d'entraînement utilisée pour calculer l'erreur naïve.\n","        seasonality (int): Saison (par exemple, 12 pour des données mensuelles avec une saison annuelle).\n","        epsilon (float): Petite valeur pour éviter la division par zéro.\n","\n","    Returns:\n","        float: Valeur du MASE.\n","    \"\"\"\n","    # Étape 1 : Calcul des prévisions naïves\n","    if len(training_series) < seasonality:\n","        # Si la série d'entraînement est plus courte que la saisonnalité, utiliser un modèle naïf basé sur un décalage minimal\n","        naive_forecasts = training_series[:-1]  # Décalage de 1 point\n","        naive_actuals = training_series[1:]    # Décalage de 1 point\n","    else:\n","        # Prévisions basées sur la saisonnalité\n","        naive_forecasts = training_series[:-seasonality]\n","        naive_actuals = training_series[seasonality:]\n","\n","    # Étape 2 : Calcul des erreurs naïves\n","    naive_errors = np.abs(naive_actuals - naive_forecasts)\n","    if len(naive_errors) == 0:\n","        raise ValueError(\"Les erreurs naïves sont vides. Vérifiez `training_series` et `seasonality`.\")\n","\n","    # Étape 3 : Calcul de l'erreur absolue moyenne (MAE) du modèle naïf\n","    mae_naive = np.mean(naive_errors)\n","    if mae_naive < epsilon:\n","        print(\"La MAE du modèle naïf est trop faible, ce qui peut entraîner une division par zéro.\")\n","\n","    # Étape 4 : Calcul du MASE\n","    mase = np.mean(np.abs(actual_values - predicted_values)) / mae_naive\n","\n","    return mase"],"metadata":{"id":"u_5qFq-5xc85"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Calcul des autres métriques"],"metadata":{"id":"UJun0UluWFN2"}},{"cell_type":"code","source":["def calculate_metrics(y_true, y_pred, epsilon=1e-8):\n","    \"\"\"\n","    Calcule les métriques : sMAPE, MSE, RMSE, msMAPE.\n","\n","    Parameters:\n","        y_true (np.ndarray): Valeurs réelles.\n","        y_pred (np.ndarray): Valeurs prédites.\n","        epsilon (float): Petite valeur pour éviter la division par zéro.\n","\n","    Returns:\n","        dict: Dictionnaire contenant les métriques calculées.\n","    \"\"\"\n","    # Assurer que y_true et y_pred sont des tableaux numpy\n","    y_true = np.asarray(y_true)\n","    y_pred = np.asarray(y_pred)\n","\n","    # Calcul du sMAPE\n","    numerator = np.abs(y_true - y_pred)\n","    denominator = np.maximum(np.abs(y_true) + np.abs(y_pred), epsilon)\n","    smape = 100 * np.mean(numerator / (denominator / 2))\n","\n","    # Calcul du MSE\n","    mse = np.mean((y_true - y_pred) ** 2)\n","\n","    # Calcul du RMSE\n","    rmse = np.sqrt(mse)\n","\n","    # Calcul du msMAPE\n","    msmapen = np.maximum(np.maximum(np.abs(y_true), np.abs(y_pred)), epsilon)\n","    msmape = np.mean(np.abs(y_true - y_pred) / msmapen)  # Moyenne des erreurs normalisées\n","\n","    # Retourner les résultats sous forme de dictionnaire\n","    return {\n","        \"sMAPE\": smape,\n","        \"MSE\": mse,\n","        \"RMSE\": rmse,\n","        \"msMAPE\": msmape,\n","    }"],"metadata":{"id":"gEzIwQ-5U8Y8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sauver les résultats dans un excel"],"metadata":{"id":"aMklpMkDJI3b"}},{"cell_type":"code","source":["def save_results_to_excel(file_path, data_dict):\n","    \"\"\"\n","    Enregistre les résultats dans un fichier Excel, en ajoutant les nouvelles lignes au fichier existant.\n","\n","    Parameters:\n","        file_path (str): Chemin vers le fichier Excel.\n","        data_dict (dict): Dictionnaire contenant les colonnes et leurs valeurs à enregistrer (clé = nom de colonne, valeur = liste).\n","    \"\"\"\n","    # Convertir le dictionnaire en DataFrame\n","    df_to_save = pd.DataFrame(data_dict)\n","\n","    # Si le fichier existe déjà, lire son contenu et ajouter les nouvelles lignes\n","    if os.path.exists(file_path):\n","        existing_df = pd.read_excel(file_path)\n","        updated_df = pd.concat([existing_df, df_to_save], ignore_index=True)\n","    else:\n","        # Si le fichier n'existe pas, créer un nouveau DataFrame\n","        updated_df = df_to_save\n","\n","    # Sauvegarder dans le fichier Excel\n","    updated_df.to_excel(file_path, index=False)"],"metadata":{"id":"ee2n4Q-vJML5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Appliquer ROCKET pour la prédiction\n","\n","Dans la section ci-dessous, nous allons utiliser toutes les fonctions définies plus haut pour appliquer l'algorithme ROCKET pour la prédiction."],"metadata":{"id":"rV5OMG4tyYai"}},{"cell_type":"code","source":["import time\n","import sys\n","\n","def apply_rocket_forecasting(path, ts_nature, num_kernels=10000,\n","                             verbose_all=False, verbose_special=False,\n","                             regressor_model='ridge', results_excel_path=\"results.xlsx\"):\n","    \"\"\"\n","    Applique ROCKET pour la tâche de forecasting sur plusieurs datasets univariés ou multivariés,\n","    calcule les métriques d'évaluation (MASE, MSE) et sauvegarde les résultats dans un fichier Excel.\n","\n","    Parameters:\n","        path (str): Chemin de base contenant les datasets.\n","        ts_nature (str): Type de série temporelle. Peut être 'univariate' ou 'multivariate'.\n","        num_kernels (int, optional): Nombre de noyaux aléatoires à générer pour ROCKET. Par défaut, 10000.\n","        verbose_all (bool, optional): Indique si tous les messages d'information sont activés. Par défaut, False.\n","        verbose_special (bool, optional): Indique si les messages spéciaux sont activés\n","            (pour les séries temporelles ayant des problèmes particuliers). Par défaut, False.\n","        regressor_model (str, optional): Modèle de régression à utiliser ('ridge', 'xgboost', ou 'randomforest'). Par défaut, 'ridge'.\n","        results_excel_path (str, optional): Chemin du fichier Excel pour enregistrer les résultats. Par défaut, \"results.xlsx\".\n","\n","    Returns:\n","        None. Les résultats sont sauvegardés dans un fichier Excel.\n","    \"\"\"\n","\n","    # Spécifier le chemin du dossier\n","    folder = 'tsf_data/' + ts_nature + '/'\n","    folder_path = path + folder\n","    if verbose_all: print(f\"Folder path: {folder_path}\")\n","\n","    # Liste tous les fichiers et sous-dossiers dans le dossier\n","    items = os.listdir(folder_path)\n","    dataset_names = [f\"{folder}{item}\" for item in items]\n","    if verbose_all: print(f\"Datasets found: {dataset_names}\")\n","\n","    # Initialiser un dictionnaire pour stocker les résultats au format requis pour l'enregistrement\n","    columns = [\"is_multivariate\", \"dataset_name\", \"forecast_horizon\", \"frequency\", \"regressor\",\n","               \"mean_mase\", \"median_mase\", \"mean_mse\", \"median_mse\", \"mean_rmse\", \"median_rmse\",\n","               \"mean_smape\", \"median_smape\", \"mean_msmape\", \"median_msmape\",\n","               \"equal_length\", \"processing_time\"]\n","    results_for_excel = {col: [] for col in columns}\n","\n","    print(\"\\nStart forecasting...\")\n","    # DEBUT BOUCLE FOR 1 sur les datasets\n","    for j in range(len(dataset_names)):\n","        dataset_name = dataset_names[j]\n","        print(\"\\nAnalyzing dataset\", dataset_name)\n","        start_time = time.time()\n","\n","        (data,\n","        frequency,\n","        forecast_horizon,\n","        contain_missing_values,\n","        contain_equal_length) = convert_tsf_to_dataframe(dataset_name,\n","                                                        replace_missing_vals_with=\"NaN\",\n","                                                        value_column_name=\"series_value\")\n","        if verbose_all: print(f\"Dataset loaded. Frequency: {frequency}, Forecast Horizon: {forecast_horizon}\")\n","\n","        # Vérifier que le dataset ne contient pas de valeurs manquantes\n","        if contain_missing_values:\n","            raise ValueError(f\"Dataset {dataset_name} contains missing values\")\n","\n","        # Déterminer l'horizon temporel pour ce dataset\n","        forecast_horizon = determine_forecast_horizon(frequency, dataset_name, forecast_horizon)\n","        if verbose_all: print(f\"Determined forecast horizon: {forecast_horizon}\")\n","\n","        # Mise en forme du dataset et préprocessing\n","        series = preprocess_series(data, dataset_name)\n","        #   si data univarié : series = liste de taille (num_examples) où chaque élément\n","        #   est une ST univariée de taille (time_length)\n","        #   si data multivarié : series = liste de taille (num_variables) où chaque\n","        #   élément est une ST univariée de taille (time_length)\n","\n","        # Initialiser des listes pour stocker les métriques pour ce dataset\n","        mase_list = []\n","        mse_list  = []\n","        rmse_list = []\n","        smape_list = []\n","        msmapen_list = []\n","\n","        # Initialiser pour le calcul du temps restant\n","        series_start_time = time.time()\n","\n","        # DEBUT BOUCLE FOR 2 sur les séries\n","        for i in range(len(series)):\n","\n","            if verbose_all: print(\"\")\n","            # ----- Affichage progression -----\n","            current_time = time.time()\n","            # Temps écoulé pour les séries déjà traitées\n","            elapsed_time = current_time - series_start_time\n","            # Temps moyen par série\n","            avg_time_per_series = elapsed_time / (i + 1)\n","            # Temps restant estimé\n","            remaining_time = avg_time_per_series * (len(series) - (i + 1))\n","            remaining_minutes, remaining_seconds = divmod(remaining_time, 60)\n","            # Affichage du compteur avec temps restant\n","            sys.stdout.write(\n","            f\"\\rProcessing series {i + 1}/{len(series)}... Estimated time left: {int(remaining_minutes)}m {int(remaining_seconds)}s\"\n","            )\n","            sys.stdout.flush()\n","            # ---------------------------------\n","            # On itère sur les séries temporelles\n","            serie = series[i]\n","\n","            # Diviser en train et en test\n","            train, test = split_time_series(serie, forecast_horizon)\n","            # Taille de `train`: (len(serie) - forecast_horizon,)\n","            # Taille de `test`: (forecast_horizon,)\n","            if verbose_all: print(f\"Data split. Train size: {len(train)}, Test size: {len(test)}\")\n","\n","            # Suivant le répertoire dans lequel est classé la série (long ou short) appliquer un traitement différent\n","            # On applique un + grand stride et taille de fenêtre pour les séries longues car sinon le\n","            # code est trop long à tourner et peut faire exploser la RAM.\n","            if 'long' in ts_nature:\n","                multiply_by = 2\n","                stride = 2\n","            else:\n","                multiply_by = 1\n","                stride = 1\n","\n","            # Calculer la taille des fenêtres glissantes (lag)\n","            min_window_size, seasonality = determine_lag(frequency, forecast_horizon, dataset_name=dataset_name, series_length=len(serie))\n","            window_size = min_window_size * multiply_by # si multiply_by = 1, min_window_size = window_size\n","            if verbose_all: print(f\"\\n\\nmin_window_size : {forecast_horizon}, window_size : {window_size}, stride : {stride}, seasonality : {seasonality}\")\n","\n","            # Vérifier la taille des fenêtres glissantes et adapter si besoin\n","            check = check_and_adjust_series(train, window_size, forecast_horizon, stride, min_window_size, min_stride=1)\n","\n","            # si la série est trop courte même après ajustement.\n","            if check['is_valid'] == False :\n","                if verbose_all or verbose_special : print(f\"WARNING for series {i} : {check['reason']}. We pass this series.\")\n","                continue  # Ignorer cette série\n","\n","            # sinon ajuster les paramètres de la série :\n","            window_size = check['adjusted_window_size'] # taille de fenêtre ajustée.\n","            stride = check['adjusted_stride'] #stride ajusté\n","            if verbose_all: print(f\"adjusted_window_size: {window_size}, adjusted_stride : {stride}\")\n","\n","            # Appliquer la fenêtre glissante\n","            X_train, y_train = sliding_window(train, window_size, forecast_horizon, stride, fill_incomplete=True)\n","            X_train = X_train.astype(np.float64)\n","            # Taille de `X_train`: (N, window_size) où N dépend de `train`, `window_size`, et `stride`\n","            # Taille de `y_train`: (N, forecast_horizon)\n","            if verbose_all: print(f\"Sliding window generated. X shape: {X_train.shape}, y shape: {y_train.shape}\")\n","\n","            input_length = X_train.shape[1]  # Longueur de la série temporelle\n","\n","            # Appliquer ROCKET\n","            if 'multivariate' in ts_nature and i == 0:\n","                kernels = generate_kernels(input_length, num_kernels)\n","\n","            if 'univariate' in ts_nature:\n","                kernels = generate_kernels(input_length, num_kernels)\n","\n","            # Extraire les caractéristiques\n","            X_train_features = apply_kernels(X_train, kernels)\n","            # Taille de `X_train_features`: (N, n_features) où `n_features` dépend du nombre de noyaux ROCKET\n","            if verbose_all: print(f\"Features extracted. Train features shape: {X_train_features.shape}\")\n","\n","            if verbose_all:\n","                # Reconstruire la série à partir des fenêtres d'entraînement (facultatif, pour validation)\n","                train_reconstructed = inverse_sliding_window(X_train, y_train, window_size, forecast_horizon, stride)\n","                # Taille de `train_reconstructed`: dépend de `X_train` et de `y_train`, approximativement égale à `train`\n","                # Comparer la reconstruction avec le train original\n","                reconstruction_valid = np.allclose(train[:len(train_reconstructed)], train_reconstructed, atol=1e-6)\n","                if reconstruction_valid:\n","                    print(\"Reconstruction 'inverse sliding windows' is correct.\")\n","                else:\n","                    print(\"Reconstruction 'inverse sliding windows' is incorrect. Verify sliding windows or parameters.\")\n","\n","            # Préparer les données de test\n","            X_test = train[-window_size:].reshape(1, -1)  # Dernières valeurs du train comme entrée\n","            X_test = X_test.astype(np.float64) # Taille de `X_test`: (1, window_size)\n","            if verbose_all: print(f\"Test data prepared. X shape: {X_test.shape}\")\n","\n","            X_test_features = apply_kernels(X_test, kernels) # Taille de `X_test_features`: (1, n_features)\n","            if verbose_all: print(f\"Features extracted. Test features shape: {X_test_features.shape}\")\n","\n","            # Appliquer un régresseur\n","            if np.var(y_train) < 1e-8:  # Vérifie si y_train est constant\n","                # Méthode de prédiction triviale si y_train constant\n","                constant_prediction = np.mean(y_train)  # Moyenne de y_train\n","                y_test_pred = np.full((1, forecast_horizon), constant_prediction)  # Remplit le forecast_horizon avec la constante\n","                if verbose_all or verbose_special:\n","                    print(f\"y_train variance too low. Using constant prediction. Constant prediction made: {constant_prediction}\")\n","            else:\n","                # Entraînement du modèle si y_train n'est pas constant\n","                y_test_pred, regressor = train_and_predict_model(X_train_features, y_train, X_test_features, model_name=regressor_model)\n","                y_test_pred = y_test_pred.flatten()\n","                if verbose_all : print(f\"Model trained. Predictions shape: {y_test_pred.shape}\")\n","\n","            if verbose_all: print(f\"actual values : {test}\\npredicted_values : {y_test_pred}\")\n","\n","            # Calculer les métriques d'évaluation\n","            mase = compute_mase(test, y_test_pred, train, seasonality)\n","            metrics = calculate_metrics(test, y_test_pred, epsilon=1e-8)\n","            if verbose_all : print(f\"MASE: {mase}, MSE: {metrics['MSE']}, RMSE : {metrics['RMSE']}, sMAPE : {metrics['sMAPE']} , msMAPE : {metrics['msMAPE']}\" )\n","\n","            # ajouter ces métriques à la liste\n","            mase_list.append(mase)\n","            mse_list.append(metrics['MSE'])\n","            rmse_list.append(metrics['RMSE'])\n","            smape_list.append(metrics['sMAPE'])\n","            msmapen_list.append(metrics['msMAPE'])\n","\n","        # FIN BOUCLE FOR 2\n","\n","        # Calculer les moyennes des métriques pour ce dataset\n","        mean_mase = np.mean(mase_list)\n","        mean_mse = np.mean(mse_list)\n","        mean_rmse = np.mean(rmse_list)\n","        mean_smape = np.mean(smape_list)\n","        mean_msmape = np.mean(msmapen_list)\n","        median_mase = np.median(mase_list)\n","        median_mse = np.median(mse_list)\n","        median_rmse = np.median(rmse_list)\n","        median_smape = np.median(smape_list)\n","        median_msmape = np.median(msmapen_list)\n","\n","        # Calcul du temps total nécessaire pour traiter une série temporelle\n","        processing_time = time.time() - start_time\n","        processing_minutes = int(processing_time // 60)  # Minutes entières\n","        processing_seconds = int(processing_time % 60)  # Secondes restantes\n","        formatted_processing_time = f\"{processing_minutes} min {processing_seconds} sec\"\n","\n","        # Ajouter les résultats pour ce dataset au dictionnaire\n","        results_for_excel[\"is_multivariate\"].append(ts_nature == \"multivariate\")\n","        results_for_excel[\"dataset_name\"].append(dataset_name)\n","        results_for_excel[\"forecast_horizon\"].append(forecast_horizon)\n","        results_for_excel[\"frequency\"].append(frequency)\n","        results_for_excel[\"regressor\"].append(regressor_model)\n","\n","        results_for_excel[\"mean_mase\"].append(mean_mase)\n","        results_for_excel[\"mean_mse\"].append(mean_mse)\n","        results_for_excel[\"mean_rmse\"].append(mean_rmse)\n","        results_for_excel[\"mean_smape\"].append(mean_smape)\n","        results_for_excel[\"mean_msmape\"].append(mean_msmape)\n","\n","        results_for_excel[\"median_mase\"].append(median_mase)\n","        results_for_excel[\"median_mse\"].append(median_mse)\n","        results_for_excel[\"median_rmse\"].append(median_rmse)\n","        results_for_excel[\"median_smape\"].append(median_smape)\n","        results_for_excel[\"median_msmape\"].append(median_msmape)\n","\n","        results_for_excel[\"equal_length\"].append(contain_equal_length)\n","        results_for_excel[\"processing_time\"].append(formatted_processing_time)\n","\n","        # Sauvegarder les résultats dans l'Excel à chaque dataset\n","        save_results_to_excel(results_excel_path, results_for_excel)\n","\n","    # FIN BOUCLE FOR 1\n","    print(\"\\n...done!\")"],"metadata":{"id":"vcRU3WsrGirN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","#warnings.resetwarnings()"],"metadata":{"id":"uHTleJvpcAPf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["apply_rocket_forecasting(path=PATH,\n","                         ts_nature='univariate/dataset_short/finish_test',\n","                         num_kernels=1000, # 10000 est le chiffre recommandé dans le Papier Rocket\n","                         verbose_all=False,\n","                         verbose_special=False,\n","                         regressor_model='ridge',\n","                         results_excel_path=\"results.xlsx\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P06GJ9F2ecBp","executionInfo":{"status":"ok","timestamp":1736285035838,"user_tz":-60,"elapsed":19552,"user":{"displayName":"Eva Robillard","userId":"06491556004018134427"}},"outputId":"50a250b3-385c-4c6a-9cbf-09d9661cf9ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Start forecasting...\n","\n","Analyzing dataset tsf_data/univariate/dataset_short/finish_test/vehicle_trips_dataset_without_missing_values.tsf\n","Processing series 100/329... Estimated time left: 0m 17sLa MAE du modèle naïf est trop faible, ce qui peut entraîner une division par zéro.\n","Processing series 267/329... Estimated time left: 0m 3sLa MAE du modèle naïf est trop faible, ce qui peut entraîner une division par zéro.\n","Processing series 284/329... Estimated time left: 0m 2sLa MAE du modèle naïf est trop faible, ce qui peut entraîner une division par zéro.\n","Processing series 329/329... Estimated time left: 0m 0s\n","...done!\n"]}]},{"cell_type":"code","source":["apply_rocket_forecasting(path=PATH,\n","                         ts_nature='univariate/dataset_long',\n","                         num_kernels=1000, # 10000 est le chiffre recommandé dans le Papier Rocket\n","                         verbose_all=False,\n","                         verbose_special=False,\n","                         regressor_model='ridge',\n","                         results_excel_path=\"results.xlsx\")"],"metadata":{"id":"3-8qLjUCJruY","colab":{"base_uri":"https://localhost:8080/","height":668},"executionInfo":{"status":"error","timestamp":1736331076347,"user_tz":-60,"elapsed":46014097,"user":{"displayName":"Eva Robillard","userId":"06491556004018134427"}},"outputId":"9c8b28d5-5ee9-4e8a-8b45-ec1436b19d05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Start forecasting...\n","\n","Analyzing dataset tsf_data/univariate/dataset_long/m4_yearly_dataset.tsf\n","Processing series 23000/23000... Estimated time left: 0m 0s\n","Analyzing dataset tsf_data/univariate/dataset_long/m4_quarterly_dataset.tsf\n","Processing series 24000/24000... Estimated time left: 0m 0s\n","Analyzing dataset tsf_data/univariate/dataset_long/m4_monthly_dataset.tsf\n","Processing series 48000/48000... Estimated time left: 0m 0s\n","Analyzing dataset tsf_data/univariate/dataset_long/m4_daily_dataset.tsf\n","Processing series 4227/4227... Estimated time left: 0m 0s\n","Analyzing dataset tsf_data/univariate/dataset_long/m4_hourly_dataset.tsf\n","Processing series 414/414... Estimated time left: 0m 0s\n","Analyzing dataset tsf_data/univariate/dataset_long/kdd_cup_2018_dataset_without_missing_values.tsf\n","Processing series 270/270... Estimated time left: 0m 0s\n","Analyzing dataset tsf_data/univariate/dataset_long/weather_dataset.tsf\n","Processing series 409/3010... Estimated time left: 1266m 50s"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-360131b26638>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m apply_rocket_forecasting(path=PATH,\n\u001b[0m\u001b[1;32m      2\u001b[0m                          \u001b[0mts_nature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'univariate/dataset_long'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mnum_kernels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 10000 est le chiffre recommandé dans le Papier Rocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mverbose_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mverbose_special\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-dd0b0c1d5e65>\u001b[0m in \u001b[0;36mapply_rocket_forecasting\u001b[0;34m(path, ts_nature, num_kernels, verbose_all, verbose_special, regressor_model, results_excel_path)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;31m# Entraînement du modèle si y_train n'est pas constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0my_test_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_predict_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregressor_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                 \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mverbose_all\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model trained. Predictions shape: {y_test_pred.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-0fa41de7cb7c>\u001b[0m in \u001b[0;36mtrain_and_predict_model\u001b[0;34m(X_train_features, y_train, X_test_features, model_name, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Ridge Regression (L2 Regularization): Gère les features corrélées sans sparsité\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidgeCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lasso'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **params)\u001b[0m\n\u001b[1;32m   2719\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m         \"\"\"\n\u001b[0;32m-> 2721\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **params)\u001b[0m\n\u001b[1;32m   2439\u001b[0m                 \u001b[0malpha_per_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_per_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2440\u001b[0m             )\n\u001b[0;32m-> 2441\u001b[0;31m             estimator.fit(\n\u001b[0m\u001b[1;32m   2442\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, score_params)\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0msqrt_sw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m         \u001b[0mX_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqrt_sw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0mn_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36m_svd_decompose_design_matrix\u001b[0;34m(self, X, y, sqrt_sw)\u001b[0m\n\u001b[1;32m   2057\u001b[0m             \u001b[0mintercept_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt_sw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0msingvals_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingvals\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m         \u001b[0mUT_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# perform decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0m\u001b[1;32m    142\u001b[0m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["apply_rocket_forecasting(path=PATH,\n","                         ts_nature='multivariate/test',\n","                         num_kernels=1000, # 10000 est le chiffre recommandé dans le Papier Rocket\n","                         verbose_all=False,\n","                         verbose_special=False,\n","                         regressor_model='ridge',\n","                         results_excel_path=\"results.xlsx\")"],"metadata":{"id":"LabM2K0SeeaK","colab":{"base_uri":"https://localhost:8080/","height":471},"executionInfo":{"status":"error","timestamp":1736335738877,"user_tz":-60,"elapsed":474,"user":{"displayName":"Eva Robillard","userId":"06491556004018134427"}},"outputId":"a3645a74-f488-4a72-98a7-3f1ee7df8ae7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Start forecasting...\n","\n","Analyzing dataset tsf_data/multivariate/test/solar_weekly_dataset.tsf\n","Processing series 1/52... Estimated time left: 0m 0s"]},{"output_type":"error","ename":"TypeError","evalue":"cannot unpack non-iterable int object","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-f19a2dcb1687>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m apply_rocket_forecasting(path=PATH,\n\u001b[0m\u001b[1;32m      2\u001b[0m                          \u001b[0mts_nature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multivariate/test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mnum_kernels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 10000 est le chiffre recommandé dans le Papier Rocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mverbose_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mverbose_special\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-dd0b0c1d5e65>\u001b[0m in \u001b[0;36mapply_rocket_forecasting\u001b[0;34m(path, ts_nature, num_kernels, verbose_all, verbose_special, regressor_model, results_excel_path)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Calculer la taille des fenêtres glissantes (lag)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mmin_window_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseasonality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermine_lag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast_horizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseries_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_window_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmultiply_by\u001b[0m \u001b[0;31m# si multiply_by = 1, min_window_size = window_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose_all\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n\\nmin_window_size : {forecast_horizon}, window_size : {window_size}, stride : {stride}, seasonality : {seasonality}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"]}]},{"cell_type":"code","source":["apply_rocket_forecasting(path=PATH,\n","                         ts_nature='univariate/dataset_short',\n","                         num_kernels=1000, # 10000 est le chiffre recommandé dans le Papier Rocket\n","                         verbose_all=False,\n","                         verbose_special=False,\n","                         regressor_model='xgboost',\n","                         results_excel_path=\"results.xlsx\")"],"metadata":{"id":"Qd1QPfBPWCJH","colab":{"base_uri":"https://localhost:8080/","height":547},"executionInfo":{"status":"error","timestamp":1736358195459,"user_tz":-60,"elapsed":7446893,"user":{"displayName":"Eva Robillard","userId":"06491556004018134427"}},"outputId":"b3973354-f30b-4982-b4cb-350bd373dad7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Start forecasting...\n","\n","Analyzing dataset tsf_data/univariate/dataset_short/m3_quarterly_dataset.tsf\n","Processing series 756/756... Estimated time left: 0m 0s\n","Analyzing dataset tsf_data/univariate/dataset_short/tourism_yearly_dataset.tsf\n","Processing series 518/518... Estimated time left: 0m 0s\n","Analyzing dataset tsf_data/univariate/dataset_short/tourism_quarterly_dataset.tsf\n","Processing series 427/427... Estimated time left: 0m 0s\n","Analyzing dataset tsf_data/univariate/dataset_short/tourism_monthly_dataset.tsf\n","Processing series 118/366... Estimated time left: 555m 12s"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-d68450f9d4fa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m apply_rocket_forecasting(path=PATH,\n\u001b[0m\u001b[1;32m      2\u001b[0m                          \u001b[0mts_nature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'univariate/dataset_short'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mnum_kernels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 10000 est le chiffre recommandé dans le Papier Rocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mverbose_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mverbose_special\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-dd0b0c1d5e65>\u001b[0m in \u001b[0;36mapply_rocket_forecasting\u001b[0;34m(path, ts_nature, num_kernels, verbose_all, verbose_special, regressor_model, results_excel_path)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;31m# Entraînement du modèle si y_train n'est pas constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0my_test_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_predict_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregressor_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                 \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mverbose_all\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model trained. Predictions shape: {y_test_pred.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-0fa41de7cb7c>\u001b[0m in \u001b[0;36mtrain_and_predict_model\u001b[0;34m(X_train_features, y_train, X_test_features, model_name, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# XGBoost: Puissant et performant, mais nécessite un ajustement d'hyperparamètres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'reg:squarederror'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1109\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             _check_call(\n\u001b[0;32m-> 2101\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["apply_rocket_forecasting(path=PATH,\n","                         ts_nature='univariate/dataset_short',\n","                         num_kernels=1000, # 10000 est le chiffre recommandé dans le Papier Rocket\n","                         verbose_all=False,\n","                         verbose_special=False,\n","                         regressor_model='random_forest',\n","                         results_excel_path=\"results.xlsx\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":547},"id":"H1YBerLhetcf","executionInfo":{"status":"error","timestamp":1736364579508,"user_tz":-60,"elapsed":2156845,"user":{"displayName":"Eva Robillard","userId":"06491556004018134427"}},"outputId":"3808b200-72e5-472b-8963-c605dd706f27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Start forecasting...\n","\n","Analyzing dataset tsf_data/univariate/dataset_short/m3_quarterly_dataset.tsf\n","Processing series 756/756... Estimated time left: 0m 0s\n","Analyzing dataset tsf_data/univariate/dataset_short/tourism_yearly_dataset.tsf\n","Processing series 518/518... Estimated time left: 0m 0s\n","Analyzing dataset tsf_data/univariate/dataset_short/tourism_quarterly_dataset.tsf\n","Processing series 427/427... Estimated time left: 0m 0s\n","Analyzing dataset tsf_data/univariate/dataset_short/tourism_monthly_dataset.tsf\n","Processing series 37/366... Estimated time left: 103m 45s"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-44d580358c53>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m apply_rocket_forecasting(path=PATH,\n\u001b[0m\u001b[1;32m      2\u001b[0m                          \u001b[0mts_nature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'univariate/dataset_short'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mnum_kernels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 10000 est le chiffre recommandé dans le Papier Rocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mverbose_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mverbose_special\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-dd0b0c1d5e65>\u001b[0m in \u001b[0;36mapply_rocket_forecasting\u001b[0;34m(path, ts_nature, num_kernels, verbose_all, verbose_special, regressor_model, results_excel_path)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;31m# Entraînement du modèle si y_train n'est pas constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0my_test_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_predict_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregressor_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                 \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mverbose_all\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model trained. Predictions shape: {y_test_pred.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-0fa41de7cb7c>\u001b[0m in \u001b[0;36mtrain_and_predict_model\u001b[0;34m(X_train_features, y_train, X_test_features, model_name, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Random Forest: Gère de nombreuses features, robuste, mais moins interprétable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'xgboost'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         tree._fit(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDvCnE2p51Nc"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}